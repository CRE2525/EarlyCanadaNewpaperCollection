{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stuff to do:\n",
    "\n",
    "#Make sure all images in 4 page papers are recieved [Done]\n",
    "#Test on other collections [Done]\n",
    "#Clean up program [Done]\n",
    "#Implement OCR [Done]\n",
    "#Implement better naming [Done kinda]\n",
    "#Make start/stop feature [Done]\n",
    "\n",
    "#Make sure all newspapers covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements \n",
    "import requests\n",
    "import time\n",
    "import ast\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import io\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_url(url: str) -> str:\n",
    "    \"\"\" Returns a list composed of the first\n",
    "    half of the given url, behind the date, and \n",
    "    the second half of the url following the \n",
    "    date. \"\"\"\n",
    "    \n",
    "    # Get indexes of halves\n",
    "    i = url.find('dat=')\n",
    "    j = url.find('&', i) \n",
    "    \n",
    "    # Slice string\n",
    "    half1 = url[:i+4]\n",
    "    half2 = url[j:]\n",
    "    \n",
    "    return [half1, half2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(str_list: list) -> list:\n",
    "    \"\"\" Returns a list containing no duplicate\n",
    "    elements, given str_list. \"\"\"\n",
    "    \n",
    "    ret_list = []\n",
    "    \n",
    "    # Iterate through items in str_list\n",
    "    for item in str_list:\n",
    "        # Check if item has been already seen\n",
    "        if item not in ret_list:\n",
    "            # If not, add to new list.\n",
    "            ret_list.append(item)\n",
    "            \n",
    "    # Return list with no duplicates\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function takes in list of hrefs to pages and gets page IDs\n",
    "def get_page_ids(href_list: list) -> list:\n",
    "    \"\"\" Returns a list of lists, each with the PIDs\n",
    "    of images located on a given news issue, given\n",
    "    a list of links to the pages. \"\"\"\n",
    "    \n",
    "    ret_list = []\n",
    "    \n",
    "    # Iterate through the list of links\n",
    "    for page in href_list:\n",
    "        \n",
    "        # Create a list for the PIDs\n",
    "        temp_list = []\n",
    "        \n",
    "        # Send request at random times\n",
    "        n = random.randint(6, 10)\n",
    "        time.sleep(n)\n",
    "        result = requests.get(page)\n",
    "        \n",
    "        print(result)\n",
    "        \n",
    "        # Get content from beautiful soup\n",
    "        if result.status_code == 200:\n",
    "            soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "\n",
    "        # Get the relevant portion of the page source\n",
    "        data = soup.find_all('script')\n",
    "        script_text = str(data[-1])\n",
    "        \n",
    "        # Use regex to find all pids in source\n",
    "        pid_list = [m.start() for m in re.finditer('\"pid\":\"', script_text)]\n",
    "\n",
    "        # Go through pids and edit them for link\n",
    "        for pid in pid_list:\n",
    "            j = script_text.find(\",\", pid)\n",
    "            i = script_text.find('\",', pid)\n",
    "            pid_num = script_text[pid+7:j] + '%2C' + script_text[j+1:i]\n",
    "            temp_list.append(pid_num)\n",
    "        \n",
    "        # Append to list to be returned\n",
    "        ret_list.append(remove_duplicates(temp_list))\n",
    "        \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets href list\n",
    "def get_links(url: str) -> list:\n",
    "    \"\"\" Returns nested list of newspaper links\n",
    "    to hrefs and image links located at url to\n",
    "    google news archive. \"\"\"\n",
    "    \n",
    "    page = url\n",
    "    \n",
    "    # Get request from url\n",
    "    n = random.randint(6, 10)\n",
    "    time.sleep(n)\n",
    "    result = requests.get(page)\n",
    "    \n",
    "    print('[attemping to reach source code at:' + url + ']')\n",
    "    print(str(result))\n",
    "    \n",
    "    # Check for non-200 status code\n",
    "    if result.status_code == 200:\n",
    "        print(\"[source code found.]\")\n",
    "        soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    else:\n",
    "        print(\"[source code not found.]\")\n",
    "        return [[],[]]\n",
    "\n",
    "    # Get needed parts of source code\n",
    "    data = soup.find_all('script')\n",
    "    script_text = str(data[2])\n",
    "\n",
    "    # Get required portion of source code as list\n",
    "    i = script_text.find('summary_data')\n",
    "    j = script_text.rfind(']')\n",
    "    script_str = script_text[i+14:j+1]\n",
    "    news_list = ast.literal_eval(script_str) \n",
    "\n",
    "    # Initiate lists to collect links\n",
    "    ret_list = []\n",
    "    href_list = []\n",
    "    img_list = []\n",
    "    \n",
    "    # Iterate through \n",
    "    for year in news_list:\n",
    "        # Check if editions exist for this year\n",
    "        if 'editions' in year:\n",
    "            year_list = year['editions']\n",
    "            # Go through editions and get image and href link\n",
    "            for edition in year_list:\n",
    "                img_list.append(edition['img_url'])\n",
    "                href_list.append(edition['href_url'])\n",
    "    \n",
    "    # Add these lists to returned list\n",
    "    ret_list.append(href_list)\n",
    "    ret_list.append(img_list)\n",
    "    \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_link_list(start_date: int, end_date: int, url: str) -> list:\n",
    "    \"\"\" Returns nested list of newspaper links\n",
    "    to hrefs and image linkes from url to google\n",
    "    news archive. from end_date to start_date\"\"\"\n",
    "    \n",
    "    # Slice and get the important parts of URL\n",
    "    sliced_url = slice_url(url)\n",
    "    \n",
    "    url1 = sliced_url[0]\n",
    "    url2 = sliced_url[1]\n",
    "    \n",
    "    href_dict = {}\n",
    "    image_dict = {}\n",
    "\n",
    "    # Go through months of archive\n",
    "    i = start_date\n",
    "    while(i < end_date):\n",
    "        # Get links from url at particular date, both at yearly mark and\n",
    "        # after sixth months\n",
    "        list_of_links1 = get_links(url1 + str(i) + url2)\n",
    "        list_of_links2 = get_links(url1 + str(i+600) + url2)\n",
    "        \n",
    "        # Add the links together\n",
    "        list_of_hrefs = list_of_links1[0] + list_of_links2[0]\n",
    "        list_of_imgs = list_of_links1[1] + list_of_links2[1]\n",
    "        \n",
    "        list_of_links = [list_of_hrefs, list_of_imgs]\n",
    "        \n",
    "        # Add the links to a dictionary so that there are no duplicates\n",
    "        for href in list_of_links[0]:\n",
    "            href_dict[href] = 1\n",
    "        for img in list_of_links[1]:\n",
    "            image_dict[img] = 1\n",
    "\n",
    "        # Forward date by a year\n",
    "        i+=10000\n",
    "        \n",
    "    # Convert keys to list and return\n",
    "    final_href_list = list(href_dict.keys())\n",
    "    final_image_list = list(image_dict.keys())\n",
    "    \n",
    "    print(\"Len of final_image_list: \" + str(len(final_image_list)))\n",
    "    print(\"Len of final_href_list: \" + str(len(final_href_list)))\n",
    "\n",
    "    return [final_href_list, final_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_id(url: str) -> str:\n",
    "    \"\"\" Return the id from the url to\n",
    "    an image link for the newspaper.\"\"\"\n",
    "\n",
    "    # Isolate and return important part of link\n",
    "    i = url.find(\"?id=\")\n",
    "    j = url.find(\"&pg=\")\n",
    "    return url[i+4:j+4]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(l: str, name: str) -> None:\n",
    "    \"\"\" Writes each item in the list\n",
    "    to a row in the text file. \"\"\"\n",
    "    \n",
    "    with open(name, \"a\") as f:\n",
    "        for item in l:\n",
    "            f.write(\"%s\\n\" % item)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_img_urls(url: str, start_date: int, end_date: int) -> list:\n",
    "    \"\"\" Returns array of template urls that point to \n",
    "    images which make up newspapers.\"\"\"\n",
    "        \n",
    "    # Get list of links to pages where newspapers and images are located\n",
    "    list_of_links = get_nested_link_list(start_date, end_date, url)\n",
    "    # Get list of PIDs for each page in newspaper\n",
    "    pid_list = get_page_ids(list_of_links[0])\n",
    "\n",
    "    final_array = []\n",
    "\n",
    "    # Iterate through the links to thumbnails\n",
    "    for i in range(len(list_of_links[1])):\n",
    "        link = list_of_links[1][i]\n",
    "        # Loop through the pids for this particular image\n",
    "        for j in range(len(pid_list[i])):\n",
    "            url1 = \"https://news.google.com/newspapers?id=\" + get_news_id(link) + pid_list[i][j] + \"&img=1&hl=en&zoom=3&tid=\"\n",
    "            final_array.append(url1)\n",
    "\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(image_url: str) -> str:\n",
    "    \"\"\" Returns name of image, downloaded \n",
    "    from image_url\"\"\"\n",
    "    \n",
    "    # Request image from URL\n",
    "    print(\"[Retrieving image from URL]\")\n",
    "    n = random.randint(6, 10)\n",
    "    time.sleep(n)\n",
    "    img_data = requests.get(image_url).content\n",
    "    \n",
    "    # Get image as string\n",
    "    data_str = str(img_data)\n",
    "    \n",
    "    # Check if image exists (it will send back a gif if not image)\n",
    "    if data_str.startswith(\"b'GIF\"):\n",
    "        print(\"Image is a gif.\")\n",
    "        return \"\"\n",
    "    \n",
    "    else:\n",
    "        # If image exists, download it to memory.\n",
    "        print(\"Image is not a gif!\")\n",
    "        with open(\"small_output/\" + image_url[-5:] + \"small_image.jpg\", \"wb\") as handler:\n",
    "            handler.write(img_data)\n",
    "        return \"small_output/\" + image_url[-5:] + \"small_image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(url: str) -> list:\n",
    "    \"\"\" Returns a list containing the names of \n",
    "    downloaded images making up image found at url.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initiate list to be returned \n",
    "    all_imgs = []\n",
    "    \n",
    "    # Set flag and iterator variable\n",
    "    found_end = False\n",
    "    i = 0\n",
    "    \n",
    "    # Collect images from link while images exist\n",
    "    while found_end == False and i < 150:\n",
    "        # Iterate through the image links, downloading them\n",
    "        send_url = url + str(i)\n",
    "        ret = get_file(send_url)\n",
    "        # If no file found set flag to True\n",
    "        if ret == \"\":\n",
    "            found_end = True\n",
    "        # Otherwise add the name of the image to the list\n",
    "        else:\n",
    "            found_end = False\n",
    "            all_imgs.append(ret)\n",
    "        i+=1\n",
    "        \n",
    "    # Return list of names\n",
    "    return all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_h_blank(im1, im2, color=(0, 0, 0)):\n",
    "    \"\"\" Merges images im1 and im2 horizontally.\n",
    "    Sourced from PIL documentation. \"\"\"\n",
    "    \n",
    "    dst = Image.new('RGB', (im1.width + im2.width, max(im1.height, im2.height)), color)\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    \n",
    "    # Returns image as object\n",
    "    return dst\n",
    "\n",
    "def get_concat_v_blank(im1, im2, color=(0, 0, 0)):\n",
    "    \"\"\" Merges images im1 and im2 vertically.\n",
    "    Sourced from PIL documentation. \"\"\"\n",
    "    \n",
    "    dst = Image.new('RGB', (max(im1.width, im2.width), im1.height + im2.height), color)\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (0, im1.height))\n",
    "    \n",
    "    # Returns image as object\n",
    "    return dst\n",
    "\n",
    "def get_concat_h_multi_blank(im_list: list, dr: str):\n",
    "    \"\"\" Merges multiple images contained in \n",
    "    im_list vertically. Sourced from PIL \n",
    "    documentation. \"\"\"\n",
    "    \n",
    "    _im = im_list.pop(0)\n",
    "    \n",
    "    # Loop through images and concatenate\n",
    "    for im in im_list:\n",
    "        if dr == \"h\":\n",
    "            _im = get_concat_h_blank(_im, im)\n",
    "        else:\n",
    "            _im = get_concat_v_blank(_im, im)\n",
    "    return _im\n",
    "\n",
    "def save_h_imgs_35(l: list) -> None:\n",
    "    \"\"\" Merges images from l in pattern which\n",
    "    creates large image for Google News Archive\n",
    "    images.\"\"\"\n",
    "    \n",
    "    get_concat_h_multi_blank([l[0], l[1], l[2], l[9], l[10]], 'h').save('h1.jpg')\n",
    "    get_concat_h_multi_blank([l[3], l[4], l[5], l[11], l[12]], 'h').save('h2.jpg')\n",
    "    get_concat_h_multi_blank([l[6], l[7], l[8], l[13], l[14]], 'h').save('h3.jpg')\n",
    "    get_concat_h_multi_blank([l[15], l[16], l[17], l[24], l[25]], 'h').save('h4.jpg')\n",
    "    get_concat_h_multi_blank([l[18], l[19], l[20], l[26], l[27]], 'h').save('h5.jpg')\n",
    "    get_concat_h_multi_blank([l[21], l[22], l[23], l[28], l[29]], 'h').save('h6.jpg')  \n",
    "    get_concat_h_multi_blank([l[30], l[31], l[32], l[33], l[34]], 'h').save('h7.jpg') \n",
    "    \n",
    "def save_h_imgs_48(l: list) -> None:\n",
    "    \"\"\" Merges images from l in pattern which\n",
    "    creates large image for Google News Archive\n",
    "    images.\"\"\"\n",
    "    \n",
    "    get_concat_h_multi_blank([l[0], l[1], l[2], l[9], l[10], l[11]], 'h').save('h1.jpg')\n",
    "    get_concat_h_multi_blank([l[3], l[4], l[5], l[12], l[13], l[14]], 'h').save('h2.jpg')\n",
    "    get_concat_h_multi_blank([l[6], l[7], l[8], l[15], l[16], l[17]], 'h').save('h3.jpg')\n",
    "    get_concat_h_multi_blank([l[18], l[19], l[20], l[27], l[28], l[29]], 'h').save('h4.jpg')\n",
    "    get_concat_h_multi_blank([l[21], l[22], l[23], l[30], l[31], l[32]], 'h').save('h5.jpg')  \n",
    "    get_concat_h_multi_blank([l[24], l[25], l[26], l[33], l[34], l[35]], 'h').save('h6.jpg') \n",
    "    get_concat_h_multi_blank([l[36], l[37], l[38], l[42], l[43], l[44]], 'h').save('h7.jpg')\n",
    "    get_concat_h_multi_blank([l[39], l[40], l[41], l[45], l[46], l[47]], 'h').save('h8.jpg')\n",
    "\n",
    "def save_h_imgs(l: list) -> None:\n",
    "    \"\"\" Merges images from l in pattern which\n",
    "    creates large image for Google News Archive\n",
    "    images.\"\"\"\n",
    "    \n",
    "    get_concat_h_multi_blank([l[0], l[1], l[2], l[9], l[10], l[11], l[18], l[19], l[20], l[27], l[28], l[29]], 'h').save('h1.jpg')\n",
    "    get_concat_h_multi_blank([l[3], l[4], l[5], l[12], l[13], l[14], l[21], l[22], l[23], l[30], l[31], l[32]], 'h').save('h2.jpg')\n",
    "    get_concat_h_multi_blank([l[6], l[7], l[8], l[15], l[16], l[17], l[24], l[25], l[26], l[33], l[34], l[35]], 'h').save('h3.jpg')\n",
    "    get_concat_h_multi_blank([l[36], l[37], l[38], l[45], l[46], l[47], l[54], l[55], l[56], l[63], l[64], l[65]], 'h').save('h4.jpg')\n",
    "    get_concat_h_multi_blank([l[39], l[40], l[41], l[48], l[49], l[50], l[57], l[58], l[59], l[66], l[67], l[68]], 'h').save('h5.jpg')\n",
    "    get_concat_h_multi_blank([l[42], l[43], l[44], l[51], l[52], l[53], l[60], l[61], l[62], l[69], l[70], l[71]], 'h').save('h6.jpg')\n",
    "    get_concat_h_multi_blank([l[72], l[73], l[74], l[81], l[82], l[83], l[90], l[91], l[92], l[99], l[100], l[101]], 'h').save('h7.jpg')\n",
    "    get_concat_h_multi_blank([l[75], l[76], l[77], l[84], l[85], l[86], l[93], l[94], l[95], l[102], l[103], l[104]], 'h').save('h8.jpg')\n",
    "    get_concat_h_multi_blank([l[78], l[79], l[80], l[87], l[88], l[89], l[96], l[97], l[98], l[105], l[106], l[107]], 'h').save('h9.jpg')\n",
    "    \n",
    "\n",
    "def save_v_img(n: int, num: int, url: str):\n",
    "    \"\"\" Given integer n, saves files named in the format\n",
    "    'h' + n + '.jpg' as a large image stacked together. Named\n",
    "    with num. \"\"\"\n",
    "    \n",
    "    saved_obj_list = []\n",
    "    for i in range(1, n):\n",
    "        im = Image.open(\"h\" + str(i) + \".jpg\")\n",
    "        saved_obj_list.append(im)\n",
    "\n",
    "    # Save with name 'h' + n + '.jpg'\n",
    "    get_concat_h_multi_blank(saved_obj_list, 'v').save('image_output/issue' + str(num) + '-' + get_news_id(url) +'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_newspaper(n: int, img_url: str):\n",
    "    \"\"\" Given img_url, generates a newspaper\n",
    "    from images collected from that url. \"\"\"\n",
    "    \n",
    "    # Generate the images composing larger image\n",
    "    img_list = generate_images(img_url)\n",
    "    obj_list = []\n",
    "    \n",
    "    # Use PIL and make each an image object\n",
    "    for file in img_list:\n",
    "        im = Image.open(file)\n",
    "        obj_list.append(im)\n",
    "    \n",
    "    # Determine what pattern the images go together with\n",
    "    list_len = len(img_list)\n",
    "    \n",
    "    # Put together each image appropriately\n",
    "    if list_len == 35:\n",
    "        save_h_imgs_35(obj_list)\n",
    "        save_v_img(8, n, img_url)\n",
    "    elif list_len == 48:\n",
    "        save_h_imgs_48(obj_list)\n",
    "        save_v_img(9, n, img_url)\n",
    "    else:\n",
    "        save_h_imgs(obj_list)\n",
    "        save_v_img(10, n, img_url)\n",
    "    \n",
    "    # Remove images from memory\n",
    "    for item in img_list:\n",
    "        os.remove(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr_data(file_name: str) -> None:\n",
    "    \n",
    "    with io.open('image_output/' + file_name + '.jpg', 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "        \n",
    "    print(\"[Running OCR]\")\n",
    "\n",
    "    image = vision.types.Image(content = content)\n",
    "    response = client.text_detection(image = image)\n",
    "    texts = response.text_annotations\n",
    "\n",
    "    # Convert the response to dictionary\n",
    "    response = MessageToDict(response)\n",
    "    \n",
    "    # Convert to json\n",
    "    j_file = 'ocr_output/' + file_name + \"_annotated_ocr.json\" \n",
    "    out_file = open(j_file, \"w\")  \n",
    "    json.dump(response, out_file)     \n",
    "    out_file.close()\n",
    "    print(\"[Saved as: \" + j_file + \"]\")\n",
    "\n",
    "    # Convert to .txt\n",
    "    if len(texts) != 0:\n",
    "        text = ('\\n\"{}\"'.format(texts[0].description))\n",
    "    else:\n",
    "        text = \"\"\n",
    "    o_file = 'ocr_output/' + file_name + \"_OCR.txt\"\n",
    "    file = open(o_file,\"w\") \n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    print(\"[Saved as: \" + o_file + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newspapers(url: str, start_date: int, end_date: int) -> None:\n",
    "    \"\"\" Generates newspapers as jpgs located\n",
    "    at the url on google's newspaper archives.\"\"\"\n",
    "    \n",
    "    # Get list of links which point to images\n",
    "    list_of_imgs = get_template_img_urls(url, start_date, end_date)\n",
    "    print(len(list_of_imgs))\n",
    "    \n",
    "    # Checkpoint by saving this list to a file\n",
    "    write_list_to_file(list_of_imgs, 'all_links.txt')\n",
    "\n",
    "    start = sum(1 for line in open('completed_images.txt'))\n",
    "    print(\"Starting at: \" + str(start))\n",
    "    # Loop through each template image link\n",
    "    with open(\"completed_images.txt\", \"a\") as g:\n",
    "        for i in range(start, len(list_of_imgs)):\n",
    "            # Print statements \n",
    "            print(\"Looking at img: \" + list_of_imgs[i])\n",
    "            print('[generating images]')\n",
    "            \n",
    "            # Create newspaper image from link\n",
    "            create_newspaper(i, list_of_imgs[i])\n",
    "            \n",
    "            # Collect OCR data\n",
    "            #get_ocr_data('issue' + str(i))\n",
    "            time.sleep(3)\n",
    "            # Log completed image\n",
    "            g.write(\"%s\\n\" % list_of_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[attemping to reach source code at:https://news.google.com/newspapers?nid=4p3FJGzxjgAC&dat=17520301&b_mode=2&hl=en]\n",
      "<Response [200]>\n",
      "[source code found.]\n",
      "[attemping to reach source code at:https://news.google.com/newspapers?nid=4p3FJGzxjgAC&dat=17520901&b_mode=2&hl=en]\n",
      "<Response [200]>\n",
      "[source code found.]\n",
      "Len of final_image_list: 45\n",
      "Len of final_href_list: 45\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "92\n",
      "Starting at: 0\n",
      "Looking at img: https://news.google.com/newspapers?id=ICY6AAAAIBAJ&sjid=HioMAAAAIBAJ&pg=0%2C0&img=1&hl=en&zoom=3&tid=\n",
      "[generating images]\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is a gif.\n",
      "Looking at img: https://news.google.com/newspapers?id=ICY6AAAAIBAJ&sjid=HioMAAAAIBAJ&pg=0%2C4775&img=1&hl=en&zoom=3&tid=\n",
      "[generating images]\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is a gif.\n",
      "Looking at img: https://news.google.com/newspapers?id=ISY6AAAAIBAJ&sjid=HioMAAAAIBAJ&pg=0%2C10093&img=1&hl=en&zoom=3&tid=\n",
      "[generating images]\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is a gif.\n",
      "Looking at img: https://news.google.com/newspapers?id=ISY6AAAAIBAJ&sjid=HioMAAAAIBAJ&pg=0%2C15522&img=1&hl=en&zoom=3&tid=\n",
      "[generating images]\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n",
      "Image is not a gif!\n",
      "[Retrieving image from URL]\n"
     ]
    }
   ],
   "source": [
    "get_newspapers(\"https://news.google.com/newspapers?nid=4p3FJGzxjgAC&dat=17520323&b_mode=2&hl=en\", 17520301, 17530101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
